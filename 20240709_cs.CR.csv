,id,title,authors,published,summary,generated_summary
0,2407.06496v1,"It's Our Loss: No Privacy Amplification for Hidden State DP-SGD With
  Non-Convex Loss",Meenatchi Sundaram Muthu Selva Annamalai,2024-07-09 01:58:19,"Differentially Private Stochastic Gradient Descent (DP-SGD) is a popular iterative algorithm used to train machine learning models while formally guaranteeing the privacy of users. However the privacy analysis of DP-SGD makes the unrealistic assumption that all intermediate iterates (aka internal state) of the algorithm are released since in practice, only the final trained model, i.e., the final iterate of the algorithm is released. In this hidden state setting, prior work has provided tighter analyses, albeit only when the loss function is constrained, e.g., strongly convex and smooth or linear. On the other hand, the privacy leakage observed empirically from hidden state DP-SGD, even when using non-convex loss functions suggest that there is in fact a gap between the theoretical privacy analysis and the privacy guarantees achieved in practice. Therefore, it remains an open question whether privacy amplification for DP-SGD is possible in the hidden state setting for general loss functions.   Unfortunately, this work answers the aforementioned research question negatively. By carefully constructing a loss function for DP-SGD, we show that for specific loss functions, the final iterate of DP-SGD alone leaks as much information as the sequence of all iterates combined. Furthermore, we empirically verify this result by evaluating the privacy leakage from the final iterate of DP-SGD with our loss function and show that this matches the theoretical upper bound guaranteed by DP exactly. Therefore, we show that the current privacy analysis fo DP-SGD is tight for general loss functions and conclude that no privacy amplification is possible for DP-SGD in general for all (possibly non-convex) loss functions.","Here is a summary of the article in Chinese:

基于机器学习模型的隐私保护 стал笔сит是在 tolerate Differential Privacy Stochastic Gradient Descent（DP-SGD）算法中广泛使用的iterative 算法，但DP-SGD的隐私分析假设给出所有中间迭代（即算法的内部状态）都被释放实际上，在实践中，只有最终训练模型，即算法的最后一Ik 目的是被释放。最近的研究表明，在限制了损失函数的条件下（例如强convex 和smooth 或线性），隐私泄露的分析已有了改进，但是实际中隐藏状态下DP-SGD的隐私泄露却证实了те奥理论隐私分析和实际隐私保障之间存在差距。因此，这个问题是否可以为DP-SGD在一般损失函数下的隐私放大的答案仍然是一个开放的问题。然而，本文回答了这个问题负面的答案。通过精心构造一个损失函数，我们表明DP-SGD的最后一个iterate独自泄露信息的幅度是全部中间iterate的累加和。同时，我们empirically 验证这个结果，通过评估DP-SGD的最后一个iterate下的隐私泄露和理论上确保的上限核实这一个结果。因此，我们表明了DP-SGD在一般损失函数下的隐私分析是紧密的，结论是DP-SGD在一般损失函数下的隐私放大不可能。"
1,2407.06552v1,"DLOVE: A new Security Evaluation Tool for Deep Learning Based
  Watermarking Techniques",Sudev Kumar Padhi; Sk. Subidh Ali,2024-07-09 05:18:14,"Recent developments in Deep Neural Network (DNN) based watermarking techniques have shown remarkable performance. The state-of-the-art DNN-based techniques not only surpass the robustness of classical watermarking techniques but also show their robustness against many image manipulation techniques. In this paper, we performed a detailed security analysis of different DNN-based watermarking techniques. We propose a new class of attack called the Deep Learning-based OVErwriting (DLOVE) attack, which leverages adversarial machine learning and overwrites the original embedded watermark with a targeted watermark in a watermarked image. To the best of our knowledge, this attack is the first of its kind. We have considered scenarios where watermarks are used to devise and formulate an adversarial attack in white box and black box settings. To show adaptability and efficiency, we launch our DLOVE attack analysis on seven different watermarking techniques, HiDDeN, ReDMark, PIMoG, Stegastamp, Aparecium, Distortion Agostic Deep Watermarking and Hiding Images in an Image. All these techniques use different approaches to create imperceptible watermarked images. Our attack analysis on these watermarking techniques with various constraints highlights the vulnerabilities of DNN-based watermarking. Extensive experimental results validate the capabilities of DLOVE. We propose DLOVE as a benchmark security analysis tool to test the robustness of future deep learning-based watermarking techniques.","以下是文章的摘要：

近年来，深度神经网络（DNN）基础水印技术取得了出色的成绩，不仅超越了传统水印技术的鲁棒性，还对很多图像处理技术具有抗性。在这篇论文中，我们conduct a detailed security analysis of various DNN-based watermarking techniques。我们提出了一种新的攻击方法，称为Deep Learning-based OVErwriting（DLOVE）攻击，它结合对抗机器学习和目标化水印将original embedded watermark overwritten in a watermarked image。这是当前唯一的攻击方法。在白盒和黑盒设置中，我们对七种不同水印技术进行了攻击分析，并在实现中展示了DLOVE的可适应性和高效性。我们的结果表明，DNN-based watermarking技术具有漏洞，DLOVE可以被用作未来的深度学习基础水印技术的行业标准安全分析工具。"
2,2407.06648v1,SEBA: Strong Evaluation of Biometric Anonymizations,Julian Todt; Simon Hanisch; Thorsten Strufe,2024-07-09 08:20:03,"Biometric data is pervasively captured and analyzed. Using modern machine learning approaches, identity and attribute inferences attacks have proven high accuracy. Anonymizations aim to mitigate such disclosures by modifying data in a way that prevents identification. However, the effectiveness of some anonymizations is unclear. Therefore, improvements of the corresponding evaluation methodology have been proposed recently. In this paper, we introduce SEBA, a framework for strong evaluation of biometric anonymizations. It combines and implements the state-of-the-art methodology in an easy-to-use and easy-to-expand software framework. This allows anonymization designers to easily test their techniques using a strong evaluation methodology. As part of this discourse, we introduce and discuss new metrics that allow for a more straightforward evaluation of the privacy-utility trade-off that is inherent to anonymization attempts. Finally, we report on a prototypical experiment to demonstrate SEBA's applicability.","文章总结：

本文介绍了一种名为SEBA的强大评估框架，旨在评估生物识别信息匿名化方案的有效性。SEBA框架结合当前的评估方法，提供了一个易于使用和扩展的软件框架，以帮助设计匿名化方案测试自己的方法。同时，文章还引入了一些新的指标，以便更好地评估匿名化尝试中的隐私-utility平衡性Finally，作者报告了一个示例实验，以证明SEBA的可行性。"
3,2407.06751v1,Laser Fault Injection Attacks against Radiation Tolerant TMR Registers,Dmytro Petryk; Zoya Dyka; Ievgen Kabin; Anselm Breitenreiter; Jan Schaeffner; Milos Krstic,2024-07-09 11:03:01,"Security requirements for the Internet of things (IoT), wireless sensor nodes, and other wireless devices connected in a network for data exchange are high. These devices are often subject to lab analysis with the objective to reveal secret hidden information. One kind of attacks to reveal the cryptographic key is to perform optical Fault Injection attacks. In this work, we investigated the IHP radiation tolerant shift registers built of Triple Modular Redundant flip-flops. In our experiments, we were able to inject different transient faults into TMR registers.","本文的摘要是：

IoT、无线传感器节点和其他无线设备在数据交换网络中连接要求高。这些设备通常会面临实验室分析以曝光秘密信息。攻击一种方法是通过光学 Fault Injection 攻擊revealing加密密钥。在本工作中，我们研究了由三重模块冗余器件组成的 IHP radiation tolerant shift registers。在我们的实验中，我们可以注入不同的瞬态故障到 TMR.core"
4,2407.06753v1,"A Comparison of Vulnerability Feature Extraction Methods from Textual
  Attack Patterns",Refat Othman; Bruno Rossi; Russo Barbara,2024-07-09 11:04:49,"Nowadays, threat reports from cybersecurity vendors incorporate detailed descriptions of attacks within unstructured text. Knowing vulnerabilities that are related to these reports helps cybersecurity researchers and practitioners understand and adjust to evolving attacks and develop mitigation plans. This paper aims to aid cybersecurity researchers and practitioners in choosing attack extraction methods to enhance the monitoring and sharing of threat intelligence. In this work, we examine five feature extraction methods (TF-IDF, LSI, BERT, MiniLM, RoBERTa) and find that Term Frequency-Inverse Document Frequency (TF-IDF) outperforms the other four methods with a precision of 75\% and an F1 score of 64\%. The findings offer valuable insights to the cybersecurity community, and our research can aid cybersecurity researchers in evaluating and comparing the effectiveness of upcoming extraction methods.","以下是文章的摘要：

当前，网络安全公司的threat reports包含详细的攻击信息，J通过前瞻性_description_attack_structure文本知道相关vulnerability的存在。Understanding和adjust to演变的攻击，并开发mitigation plans的意识。这篇论文目的是帮助网络安全研究者和实践者选择攻击抽取方法，以提高威胁智能的监控和共享。我们在这项工作中调查了五种功能抽取方法（TF-IDF、LSI、BERT、MiniLM、RoBERTa），发现Term Frequency-Inverse Document Frequency（TF-IDF）以75%的精度和64%的F1分数击败其他四种方法。我们的发现对网络安全社区有价值，我们的研究可以帮助网络安全研究者评估和比较即将到的抽取方法的有效性。"
5,2407.06758v1,"On the Influence of the Laser Illumination on the Logic Cells Current
  Consumption",Dmytro Petryk; Zoya Dyka; Milos Krstic; Jan Bělohoubek; Petr Fišer; František Steiner; Tomáš Blecha; Peter Langendörfer; Ievgen Kabin,2024-07-09 11:08:23,"Physical side-channel attacks represent a great challenge for today's chip design. Although attacks on CMOS dynamic power represent a class of state-of-the-art attacks, many other effects potentially affect the security of CMOS chips analogously by affecting mostly static behaviour of the chip, including aging, ionizing radiation, or non-ionizing illumination of the CMOS. Vulnerabilities exploiting data dependency in CMOS static power were already demonstrated in practice and the analogous vulnerability exploiting light-modulated static power was demonstrated by simulation. This work confirms the CMOS vulnerability related to the light-modulated data-dependent static power experimentally and discusses future work.","以下是文章的摘要：

物理侧-channel攻击当前是超大规模集成电路的设计挑战。这篇文章指出，攻击CMOS动态电力只是当前攻击的一种，而其他许多effects可能会影响CMOS芯片的安全，如CMOS芯片的老化、照明放射或非照明照明。此外，文章还指出，已有证据表明CMOS芯片的数据依赖性攻击该突出该攻击的可能性，且已在通过模拟中证明该攻击。"
6,2407.06759v1,"Cybersecurity Defenses: Exploration of CVE Types through Attack
  Descriptions",Refat Othman; Bruno Rossi; Barbara Russo,2024-07-09 11:08:35,"Vulnerabilities in software security can remain undiscovered even after being exploited. Linking attacks to vulnerabilities helps experts identify and respond promptly to the incident. This paper introduces VULDAT, a classification tool using a sentence transformer MPNET to identify system vulnerabilities from attack descriptions. Our model was applied to 100 attack techniques from the ATT&CK repository and 685 issues from the CVE repository. Then, we compare the performance of VULDAT against the other eight state-of-the-art classifiers based on sentence transformers. Our findings indicate that our model achieves the best performance with F1 score of 0.85, Precision of 0.86, and Recall of 0.83. Furthermore, we found 56% of CVE reports vulnerabilities associated with an attack were identified by VULDAT, and 61% of identified vulnerabilities were in the CVE repository.","以下是文章的摘要：

该论文介绍了一款名为VULDAT的分类工具，这款工具使用(sentence transformer MPNET)来识别攻击描述中隐藏的系统漏洞。论文对VULDAT进行了评估，使用了ATT&CK仓库中的100种攻击技术和CVE仓库中的685个问题，并与8个state-of-the-art分类器进行比较。结果表明，VULDAT的性能最好，F1 score达到0.85，Precision达到0.86，Recall达到0.83。此外，VULDAT还能够识别56%的CVE报告中的漏洞，并且61%的识别漏洞来自CVE仓库。"
7,2407.06760v1,"On the Importance of Reproducibility of Experimental Results Especially
  in the Domain of Security",Dmytro Petryk; Ievgen Kabin; Peter Langendörfer; Zoya Dyka,2024-07-09 11:12:14,"Security especially in the fields of IoT, industrial automation and critical infrastructure is paramount nowadays and a hot research topic. In order to ensure confidence in research results they need to be reproducible. In the past we reported [18] that in many publications important information such as details about the equipment used are missing. In this paper we report on our own experiments that we run to verify the parameters reported in the datasheets that came along with our experimental equipment. Our results show that there are significant discrepancies between the datasheets and the real world data. These deviations concern accuracy of positions, movements, duration of laser shots etc. In order to improve reproducibility of results we therefore argue on the one hand that research groups verify the data given in datasheets of equipment they use and on the other hand that they provide measurement set-up parameters in globally accepted units such as cm, seconds, etc.","本篇文章的摘要是：

在 IoT、工业自动化和关键基础设施等领域安全问题确立虚高的研究课题。在确保研究结果的可靠性方面，回溯性变得非常重要。文章作者在过去报告过，对于许多论文不得不缺少设备细节。在本文中，我们报告了自己关于实验设备datasheet中参数的验证实验结果。结果表明，datasheet和实际世界数据存在较大的偏差，这些偏差涉及到位置准确性、运动时长、激光射频等方面。为了提高结果可靠性，我们提出了两个建议：一是研究团队需要验证使用的设备datasheet中的数据；二是他们需要提供使用测量参数，以自然 Accepted units，如cm、秒等。"
8,2407.06778v1,A BERT-based Empirical Study of Privacy Policies' Compliance with GDPR,Lu Zhang; Nabil Moukafih; Hamad Alamri; Gregory Epiphaniou; Carsten Maple,2024-07-09 11:47:52,"Since its implementation in May 2018, the General Data Protection Regulation (GDPR) has prompted businesses to revisit and revise their data handling practices to ensure compliance. The privacy policy, which serves as the primary means of informing users about their privacy rights and the data practices of companies, has been significantly updated by numerous businesses post-GDPR implementation. However, many privacy policies remain packed with technical jargon, lengthy explanations, and vague descriptions of data practices and user rights. This makes it a challenging task for users and regulatory authorities to manually verify the GDPR compliance of these privacy policies. In this study, we aim to address the challenge of compliance analysis between GDPR (Article 13) and privacy policies for 5G networks. We manually collected privacy policies from almost 70 different 5G MNOs, and we utilized an automated BERT-based model for classification. We show that an encouraging 51$\%$ of companies demonstrate a strong adherence to GDPR. In addition, we present the first study that provides current empirical evidence on the readability of privacy policies for 5G network. we adopted readability analysis toolset that incorporates various established readability metrics. The findings empirically show that the readability of the majority of current privacy policies remains a significant challenge. Hence, 5G providers need to invest considerable effort into revising these documents to enhance both their utility and the overall user experience.","本文总结：

自2018年5月实施该欧洲通用数据保护规则（GDPR）以来，公司开始重新检查和修改数据处理实践以确保符合法规。禁止太多公司重新编写隐私政策，使用户了解他们关于隐私权和公司数据处理的信息。然而，许多隐私政策仍然包含技术术语、详细解释和模糊描述数据处理和用户权利。但是，这使得用户和监管机构非常困难地验证这些隐私政策是否符合GDPR。文章中，我们 manual 70多家5G移动网络运营商隐私政策，并使用基于BERT的自动模型分类。结果表明，51%的公司对GDPR表示 थJunior进一步，我们也首次提供了5G网络当前隐私政策可读性数据，我们使用各种可读性指标工具套件进行分析。结果证明，多数当前隐私政策的可读性仍然是一个大的挑战。因此，5G提供商需要尽毕力地重新编写这些文件以提高其实用性和用户体验。"
9,2407.06838v1,Event Trojan: Asynchronous Event-based Backdoor Attacks,Ruofei Wang; Qing Guo; Haoliang Li; Renjie Wan,2024-07-09 13:15:39,"As asynchronous event data is more frequently engaged in various vision tasks, the risk of backdoor attacks becomes more evident. However, research into the potential risk associated with backdoor attacks in asynchronous event data has been scarce, leaving related tasks vulnerable to potential threats. This paper has uncovered the possibility of directly poisoning event data streams by proposing Event Trojan framework, including two kinds of triggers, i.e., immutable and mutable triggers. Specifically, our two types of event triggers are based on a sequence of simulated event spikes, which can be easily incorporated into any event stream to initiate backdoor attacks. Additionally, for the mutable trigger, we design an adaptive learning mechanism to maximize its aggressiveness. To improve the stealthiness, we introduce a novel loss function that constrains the generated contents of mutable triggers, minimizing the difference between triggers and original events while maintaining effectiveness. Extensive experiments on public event datasets show the effectiveness of the proposed backdoor triggers. We hope that this paper can draw greater attention to the potential threats posed by backdoor attacks on event-based tasks. Our code is available at https://github.com/rfww/EventTrojan.","Here is a summary of the article in Chinese:

隨著異步事件數據在視觉任务中的应用成为了日常先锋，但是關於異步事件數據中的後門攻击风险的研究发掘仍然是相对缺乏的，這使原本可能的任务变得ulnerable對于潜在的威胁。在本文中，我们提出了Event Trojan framework，它可以直接毒害事件數據流，並且提供了两个種類的触发器，即不可变触发器和可变触发器。可变触发器中，我们设计了一個自适应学习機制以最大化其攻擊力。為了提高隐蔽性，我們引入了一個 novel損失函數，该函數 最小化了trigger生成的内容與原始事件之間的差异，同时保持了攻擊力的效果。實驗結果證明了提出的後門触发器的有效性。我們希望這篇文章可以引起更多人的关注，關於異步事件數據中的潜在威胁。"
10,2407.06853v1,"TimeTravel: Real-time Timing Drift Attack on System Time Using Acoustic
  Waves",Jianshuo Liu; Hong Li; Haining Wang; Mengjie Sun; Hui Wen; Jinfa Wang; Limin Sun,2024-07-09 13:41:46,"Real-time Clock (RTC) has been widely used in various real-time systems to provide precise system time. In this paper, we reveal a new security vulnerability of the RTC circuit, where the internal storage time or timestamp can be arbitrarily modified forward or backward. The security threat of dynamic modifications of system time caused by this vulnerability is called TimeTravel. Based on acoustic resonance and piezoelectric effects, TimeTravel applies acoustic guide waves to the quartz crystal, thereby adjusting the characteristics of the oscillating signal transmitted into the RTC circuit. By manipulating the parameters of acoustic waves, TimeTravel can accelerate or decelerate the timing speed of system time at an adjustable rate, resulting in the relative drift of the timing, which can pose serious safety threats. To assess the severity of TimeTravel, we examine nine modules and two commercial devices under the RTC circuit. The experimental results show that TimeTravel can drift system time forward and backward at a chosen speed with a maximum 93% accuracy. Our analysis further shows that TimeTravel can maintain an attack success rate of no less than 77% under environments with typical obstacle items.","以下是文章的摘要：

文章揭露了一种关于实时钟（RTC）宿主库存时或时间戳可以被任意修改的新的安全漏洞，这种漏洞称为 TimeTravel。TimeTravel通过使用声学共振和磁电效果，使用声学引导波调整石英晶体的振动信号，从而调整系统时间的计时速度。实验结果表明，TimeTravel可以在chosen速度下.drift系统时间，攻击成功率高达93%。此外，TimeTravel在典型的障碍物环境下也可以保持至少77%的攻击成功率。"
11,2407.06855v1,"Performance Evaluation of Knowledge Graph Embedding Approaches under
  Non-adversarial Attacks",Sourabh Kapoor; Arnab Sharma; Michael Röder; Caglar Demir; Axel-Cyrille Ngonga Ngomo,2024-07-09 13:42:14,"Knowledge Graph Embedding (KGE) transforms a discrete Knowledge Graph (KG) into a continuous vector space facilitating its use in various AI-driven applications like Semantic Search, Question Answering, or Recommenders. While KGE approaches are effective in these applications, most existing approaches assume that all information in the given KG is correct. This enables attackers to influence the output of these approaches, e.g., by perturbing the input. Consequently, the robustness of such KGE approaches has to be addressed. Recent work focused on adversarial attacks. However, non-adversarial attacks on all attack surfaces of these approaches have not been thoroughly examined. We close this gap by evaluating the impact of non-adversarial attacks on the performance of 5 state-of-the-art KGE algorithms on 5 datasets with respect to attacks on 3 attack surfaces-graph, parameter, and label perturbation. Our evaluation results suggest that label perturbation has a strong effect on the KGE performance, followed by parameter perturbation with a moderate and graph with a low effect.","这篇文章的摘要是：

知识图嵌入(Knowledge Graph Embedding)将知识图变换为连续向量空间，用于语义搜索、问答和推荐等应用。但是，这些方法假设知识图中的所有信息都是正确的，这使得攻击者可以影响这些方法的输出。因此，需要 adres 实现在知识图嵌入中的鲁棒性。近期研究主要集中在攻击上，但还没有完整地涵盖这些方法的所有攻击面。该paper填补了这个gap，评估了5种state-of-the-art KGE算法在5个数据集中的性能，对于3种攻击面（图形、参数、标签扰动）进行了评估。结果表明，标签扰动对KGE性能的影响最强，参数扰动具有中等影响，而图形扰动的影响最弱。"
12,2407.06911v1,Differentially Private Multiway and $k$-Cut,Rishi Chandra; Michael Dinitz; Chenglin Fan; Zongrui Zou,2024-07-09 14:46:33,"In this paper, we address the challenge of differential privacy in the context of graph cuts, specifically focusing on the minimum $k$-cut and multiway cut problems. We introduce edge-differentially private algorithms that achieve nearly optimal performance for these problems.   For the multiway cut problem, we first provide a private algorithm with a multiplicative approximation ratio that matches the state-of-the-art non-private algorithm. We then present a tight information-theoretic lower bound on the additive error, demonstrating that our algorithm on weighted graphs is near-optimal for constant $k$. For the minimum $k$-cut problem, our algorithms leverage a known bound on the number of approximate $k$-cuts, resulting in a private algorithm with optimal additive error $O(k\log n)$ for fixed privacy parameter. We also establish a information-theoretic lower bound that matches this additive error. Additionally, we give an efficient private algorithm for $k$-cut even for non-constant $k$, including a polynomial-time 2-approximation with an additive error of $\widetilde{O}(k^{1.5})$.","这篇文章主要讨论了图形切分（Graph Cuts）中的差异隱私（Differential Privacy）挑战，并提出了 nearly optimal 性能的边缘隱私算法 สำหร杰k-cut和多路切分问题。

对多路切分问题，文章首先提出了一个私有算法，该算法的乘法近似比（Multiplicative Approximation Ratio）与当前非私有算法一致。此外，文章还提供了关于加法误差的严格信息理论下限，表明了我们的算法在_weighted graphs_中对 constant $k$ 的近最优性。

对于最小 $k$-cut 问题，文章的算法利用了对近似$k$-cuts 的数量的已知界，因此可以在固定隐私参数下获得 $O(k\log n)$ 的私有算法对应的加法误差。此外，文章还提供了一致的信息理论下限 对于固定隐私参数的私有算法加法误差。

最后，文章还给出了一个高效的私有算法 对于 non-constant $k$ 的 $k$-cut 问题，证明了双线性近似算法（2-approximation）有一个加法误差为 $\widetilde{O}(k^{1.5})$。"
13,2407.06942v1,An Improved Two-Step Attack on CRYSTALS-Kyber,Kai Wang; Dejun Xu; Jing Tian,2024-07-09 15:19:09,"After three rounds of post-quantum cryptography (PQC) strict evaluations conducted by the national institute of standards and technology (NIST), CRYSTALS-Kyber has successfully been selected and drafted for standardization from the mid of 2022. It becomes urgent to further evaluate Kyber's physical security for the upcoming deployment phase. In this paper, we present an improved two-step attack on Kyber to quickly recover the full secret key, s, by using much fewer energy traces and less time. In the first step, we use the correlation power analysis (CPA) attack to obtain a portion of guess values of s with a small number of energy traces. The CPA attack is enhanced by utilizing both the Pearson and Kendall's rank correlation coefficients and modifying the leakage model to improve the accuracy. In the second step, we adopt the lattice attack to recover s based on the results of CPA. The success rate is largely built up by constructing a trail-and-error method. We implement the proposed attack for the reference implementation of Kyber512 (4 128-value groups of s) on ARM Cortex-M4 and successfully recover a 128-value group of s in about 9 minutes using a 16-core machine. Additionally, in that case, we only cost at most 60 CPA guess values for a group and 15 power traces for a guess.","Here is a summary of the article in Chinese:

短评文章中介绍了CRYSTALS-Kyber对平成量密码（Post-Quantum Cryptography，PQC）的评估和选拔。然而，即将部署时，Kyber的物理安全性仍需要review。文章中提出了一种新型的两步攻击对Kyber进行攻击。首先，使用相互关联权度分析（Correlation Power Analysis，CPA）获得关于秘密密钥的部分猜测结果，然后使用立方体攻击来恢复秘密密钥。使用 Pearson 和 Kendall 的排名相关系数来提高 CPA 的准确性，并对泄露模型进行修改。实验结果表明，在 ARM Cortex-M4 上，拿走一个 16 核机器，约 9 分钟可以恢复一个 128 值组的秘密密钥，且仅需 60 个 CPA 猜测结果和 15 个能量痕迹。"
14,2407.06955v1,"ICLGuard: Controlling In-Context Learning Behavior for Applicability
  Authorization",Wai Man Si; Michael Backes; Yang Zhang,2024-07-09 15:35:06,"In-context learning (ICL) is a recent advancement in the capabilities of large language models (LLMs). This feature allows users to perform a new task without updating the model. Concretely, users can address tasks during the inference time by conditioning on a few input-label pair demonstrations along with the test input. It is different than the conventional fine-tuning paradigm and offers more flexibility. However, this capability also introduces potential issues. For example, users may use the model on any data without restriction, such as performing tasks with improper or sensitive content, which might violate the model policy or conflict with the model owner's interests. As a model owner, it is crucial to establish a mechanism to control the model's behavior under ICL, depending on the model owner's requirements for various content. To this end, we introduce the concept of ""applicability authorization"" tailored for LLMs, particularly for ICL behavior, and propose a simple approach, ICLGuard. It is a fine-tuning framework designed to allow the model owner to regulate ICL behavior on different data. ICLGuard preserves the original LLM and fine-tunes only a minimal set of additional trainable parameters to ""guard"" the LLM. Empirical results show that the guarded LLM can deactivate its ICL ability on target data without affecting its ICL ability on other data and its general functionality across all data.",本文讨论了大语言模型（LLMs）的新增功能：in-context learning（ICL）。ICL允许用户在模型未更新的情况下执行新的任务，只需在测试输入中提供少数输入-标签对示例。虽然ICL提供了可灵活性，但同时也带来了潜在问题，例如模型可能被用于任何数据，包括不恰当或敏感内容，从而违反模型策略或-owner interes्ट。为解决这个问题，作者提出了适用于LLMs的概念“applicability authorization”和ICLGuard，就是一个微调框架，允许模型所有者在不同数据上控制ICL行为。实验结果表明， guarded LLM可以在目标数据上禁用ICL能力，而不会影响其在其他数据上的ICL能力或总体功能。
15,2407.07054v1,"A Differentially Private Blockchain-Based Approach for Vertical
  Federated Learning",Linh Tran; Sanjay Chari; Md. Saikat Islam Khan; Aaron Zachariah; Stacy Patterson; Oshani Seneviratne,2024-07-09 17:20:49,"We present the Differentially Private Blockchain-Based Vertical Federal Learning (DP-BBVFL) algorithm that provides verifiability and privacy guarantees for decentralized applications. DP-BBVFL uses a smart contract to aggregate the feature representations, i.e., the embeddings, from clients transparently. We apply local differential privacy to provide privacy for embeddings stored on a blockchain, hence protecting the original data. We provide the first prototype application of differential privacy with blockchain for vertical federated learning. Our experiments with medical data show that DP-BBVFL achieves high accuracy with a tradeoff in training time due to on-chain aggregation. This innovative fusion of differential privacy and blockchain technology in DP-BBVFL could herald a new era of collaborative and trustworthy machine learning applications across several decentralized application domains.","Here is a summary of the article in Chinese:

我们的论文proposal了一种名为DP-BBVFL（Differentially Private Blockchain-Based Vertical Federated Learning）的算法，这种算法为基于区块链的应用程序提供了可靠性和隐私保护。DP-BBVFL使用智能合约将客户端的特征表示（embedding）聚合透明地。我们使用局部差分隐私来保护存储在区块链中的Embedding，保护原始数据不被泄露。我们提供了基于区块链的垂直联邦学习中第一次隐私保护的示例应用程序。我们的实验表明，DP-BBVFL在medical数据上的准确率很高，且由于区块链聚合會受到一定的训练时间影响。DP-BBVFL将不同领域的协作机器学习应用程序带入了一个新的时代。"
16,2407.07064v1,"Prompting Techniques for Secure Code Generation: A Systematic
  Investigation",Catherine Tony; Nicolás E. Díaz Ferreyra; Markus Mutas; Salem Dhiff; Riccardo Scandariato,2024-07-09 17:38:03,"Large Language Models (LLMs) are gaining momentum in software development with prompt-driven programming enabling developers to create code from natural language (NL) instructions. However, studies have questioned their ability to produce secure code and, thereby, the quality of prompt-generated software. Alongside, various prompting techniques that carefully tailor prompts have emerged to elicit optimal responses from LLMs. Still, the interplay between such prompting strategies and secure code generation remains under-explored and calls for further investigations. OBJECTIVE: In this study, we investigate the impact of different prompting techniques on the security of code generated from NL instructions by LLMs. METHOD: First we perform a systematic literature review to identify the existing prompting techniques that can be used for code generation tasks. A subset of these techniques are evaluated on GPT-3, GPT-3.5, and GPT-4 models for secure code generation. For this, we used an existing dataset consisting of 150 NL security-relevant code-generation prompts. RESULTS: Our work (i) classifies potential prompting techniques for code generation (ii) adapts and evaluates a subset of the identified techniques for secure code generation tasks and (iii) observes a reduction in security weaknesses across the tested LLMs, especially after using an existing technique called Recursive Criticism and Improvement (RCI), contributing valuable insights to the ongoing discourse on LLM-generated code security.","Here is a summary of the article in Chinese:

此研究的目的是探讨不同的(prompt)技术如何影响 Language Large Models(LLMs)根据自然语言(NL)指令生成的安全代码。作者通过系统文献综述识别了可用于代码生成任务的一些(prompt)技术，并在 GPT-3、GPT-3.5 和 GPT-4 模型上对其中的一些技术进行了评估。作者使用 existed于 150 个 NL 安全相关代码生成 PROMPT。结果表明，LLMs 中的(prompt)技术可以有效地减少安全弱点，特别是在使用一种叫做 Recursive Criticism and Improvement(RCI) 的技术时。"
17,2407.07066v2,"Explainable Hyperdimensional Computing for Balancing Privacy and
  Transparency in Additive Manufacturing Monitoring",Fardin Jalil Piran; Prathyush P. Poduval; Hamza Errahmouni Barkam; Mohsen Imani; Farhad Imani,2024-07-09 17:42:26,"In-situ sensing, in conjunction with learning models, presents a unique opportunity to address persistent defect issues in Additive Manufacturing (AM) processes. However, this integration introduces significant data privacy concerns, such as data leakage, sensor data compromise, and model inversion attacks, revealing critical details about part design, material composition, and machine parameters. Differential Privacy (DP) models, which inject noise into data under mathematical guarantees, offer a nuanced balance between data utility and privacy by obscuring traces of sensing data. However, the introduction of noise into learning models, often functioning as black boxes, complicates the prediction of how specific noise levels impact model accuracy. This study introduces the Differential Privacy-HyperDimensional computing (DP-HD) framework, leveraging the explainability of the vector symbolic paradigm to predict the noise impact on the accuracy of in-situ monitoring, safeguarding sensitive data while maintaining operational efficiency. Experimental results on real-world high-speed melt pool data of AM for detecting overhang anomalies demonstrate that DP-HD achieves superior operational efficiency, prediction accuracy, and robust privacy protection, outperforming state-of-the-art Machine Learning (ML) models. For example, when implementing the same level of privacy protection (with a privacy budget set at 1), our model achieved an accuracy of 94.43%, surpassing the performance of traditional models such as ResNet50 (52.30%), GoogLeNet (23.85%), AlexNet (55.78%), DenseNet201 (69.13%), and EfficientNet B2 (40.81%). Notably, DP-HD maintains high performance under substantial noise additions designed to enhance privacy, unlike current models that suffer significant accuracy declines under high privacy constraints.","Here is a summary of the article in Chinese:

ADDITIVE製造（AM）過程中，Through in-situ sensing and learning models, persistent defect issues can be addressed. However, this integration raises significant data privacy concerns, such as data leakage, sensor data compromise, and model inversion attacks, revealing critical details about part design, material composition, and machine parameters. Differential Privacy（DP）models can balance data utility and privacy by injecting noise into data. However, this noise affects learning model accuracy, making it difficult to predict. To address this issue, the DP-HD framework is introduced, using the vector symbolic paradigm to predict the impact of noise on accuracy while safeguarding sensitive data. Experimental results on real-world high-speed melt pool data of AM demonstrate that DP-HD achieves superior operational efficiency, prediction accuracy, and robust privacy protection, outperforming state-of-the-art Machine Learning（ML）models."
18,2407.07166v1,"UEFI Vulnerability Signature Generation using Static and Symbolic
  Analysis",Md Shafiuzzaman; Achintya Desai; Laboni Sarker; Tevfik Bultan,2024-07-09 18:08:49,"Since its major release in 2006, the Unified Extensible Firmware Interface (UEFI) has become the industry standard for interfacing a computer's hardware and operating system, replacing BIOS. UEFI has higher privileged security access to system resources than any other software component, including the system kernel. Hence, identifying and characterizing vulnerabilities in UEFI is extremely important for computer security. However, automated detection and characterization of UEFI vulnerabilities is a challenging problem. Static vulnerability analysis techniques are scalable but lack precision (reporting many false positives), whereas symbolic analysis techniques are precise but are hampered by scalability issues due to path explosion and the cost of constraint solving. In this paper, we introduce a technique called STatic Analysis guided Symbolic Execution (STASE), which integrates both analysis approaches to leverage their strengths and minimize their weaknesses. We begin with a rule-based static vulnerability analysis on LLVM bitcode to identify potential vulnerability targets for symbolic execution. We then focus symbolic execution on each target to achieve precise vulnerability detection and signature generation. STASE relies on the manual specification of reusable vulnerability rules and attacker-controlled inputs. However, it automates the generation of harnesses that guide the symbolic execution process, addressing the usability and scalability of symbolic execution, which typically requires manual harness generation to reduce the state space. We implemented and applied STASE to the implementations of UEFI code base. STASE detects and generates vulnerability signatures for 5 out of 9 recently reported PixieFail vulnerabilities and 13 new vulnerabilities in Tianocore's EDKII codebase.","Here is a summary of the article in Chinese:

从2006年开始，统一可扩展firmware接口（UEFI）成为计算机硬件和操作系统之间的标准接口，取代BIOS。UEFI具有更高的系统资源访问权限ヌuany other software component, including the system kernel。因此，鉴定和 characterize UEFI vulnerabilities 对于计算机安全非常重要。然而，自动检测和 characterizing UEFI vulnerabilities 是一个挑战性问题。静态 vulnerability análisis技术是可扩展的，但缺乏精度（报告许多假阳性），而象形分析技术是精准的，但受path explosion和约束解决方案的成本所限。在本文中，我们介绍了一种技术_called STatic Analysis guided Symbolic Execution（STASE），将两种分析方法集成，以发挥其优势和最小化其劣势。首先，在LLVM bitcode上使用规则定制的静态 vulnerability analysisRecognition来识别潜在的漏洞目标，then focus symbolic execution on each target to achieve precise vulnerability detection and signature generation。 STASE基于可重复的漏洞规则和攻击者控制的输入，但自动化Generates监管 harnesses，解决symbolic execution的可用性和可扩展性问题。我们实现并应用STASE到UEFI代码库的实现中。STASE detect和generateavailability签名出5 out of 9 recently reported PixieFail vulnerabilities和13 new vulnerabilities in Tianocore's EDKII codebase。"
19,2407.07205v1,"The Emperor is Now Clothed: A Secure Governance Framework for Web User
  Authentication through Password Managers",Ali Cherry; Konstantinos Barmpis; Siamak F. Shahandashti,2024-07-09 19:49:49,"Existing approaches to facilitate the interaction between password managers and web applications fall short of providing adequate functionality and mitigation strategies against prominent attacks. HTML Autofill is not sufficiently expressive, Credential Management API does not support browser extension password managers, and other proposed solutions do not conform to established user mental models. In this paper, we propose Berytus, a browser-based governance framework that mediates the interaction between password managers and web applications. Two APIs are designed to support Berytus acting as an orchestrator between password managers and web applications. An implementation of the framework in Firefox is developed that fully supports registration and authentication processes. As an orchestrator, Berytus is able to authenticate web applications and facilitate authenticated key exchange between web applications and password managers, which as we show, can provide effective mitigation strategies against phishing, cross-site scripting, inline code injection (e.g., by a malicious browser extension), and TLS proxy in the middle attacks, whereas existing mitigation strategies such as Content Security Policy and credential tokenisation are only partially effective. The framework design also provides desirable functional properties such as support for multi-step, multi-factor, and custom authentication schemes. We provide a comprehensive security and functionality evaluation and discuss possible future directions.","本文的摘要是：

本文提出了浏览器中的一种新框架Berytus，旨在加强密码管理器和 web 应用程序之间的交互。该框架设计了两个 API，以便在浏览器中ْف达到密码管理器和 web 应用程序之间的协作。实际上，在 Firefox 中实现了该框架的注册和认证过程。Berytus 作为调协员，可以认证 web 应用程序，并使它们之间实现加密的密钥交换，这可以有效地防止 phishing、跨站脚本、inline 代码注入和 TLS 代理中间攻击。相比之下，现有的安全策略，如内容安全策略和Credential tokenisation，仅部分有效。该框架设计还具有多步、多因素和自定义认证方案的支持性功能。"
20,2407.07221v1,"Tracing Back the Malicious Clients in Poisoning Attacks to Federated
  Learning",Yuqi Jia; Minghong Fang; Hongbin Liu; Jinghuai Zhang; Neil Zhenqiang Gong,2024-07-09 20:35:36,"Poisoning attacks compromise the training phase of federated learning (FL) such that the learned global model misclassifies attacker-chosen inputs called target inputs. Existing defenses mainly focus on protecting the training phase of FL such that the learnt global model is poison free. However, these defenses often achieve limited effectiveness when the clients' local training data is highly non-iid or the number of malicious clients is large, as confirmed in our experiments. In this work, we propose FLForensics, the first poison-forensics method for FL. FLForensics complements existing training-phase defenses. In particular, when training-phase defenses fail and a poisoned global model is deployed, FLForensics aims to trace back the malicious clients that performed the poisoning attack after a misclassified target input is identified. We theoretically show that FLForensics can accurately distinguish between benign and malicious clients under a formal definition of poisoning attack. Moreover, we empirically show the effectiveness of FLForensics at tracing back both existing and adaptive poisoning attacks on five benchmark datasets.","Here is a summary of the article in Chinese:

 Federated Learning（FL）中的 Poisoning攻擊種植業訓練階段，從而導致全球模型誤分攻擊者選擇的輸入稱為目標輸入。目前的防禦針對FL的訓練階段，以確保學習到的全球模型不受毒害。然而，這些防禦措施通常在客戶的本地訓練數據非常不一致或惡意客戶عداد很大時，表現不佳，這在我們的實驗中經證實。在這篇文章中，我們提出FLForensics，FL中第一個Poison-Forensics方法。FLForensics补充現有的.training期防禦措施。在特定情況下，如果training期防禦措施失敗，Deployment了毒害的全球模型，FLForensics旨在追溯到執行攻擊的惡意客戶 Ridley了一個誤分的目標輸入後。理论上，我們證明FLForensics可以在毒害攻擊的公式定義下精確區分善良和惡意客戶。此外，小平German面對五個基准數據集進行實驗證明FLForensics的有效性。"
21,2407.07237v1,"The Quantum Imitation Game: Reverse Engineering of Quantum Machine
  Learning Models",Archisman Ghosh; Swaroop Ghosh,2024-07-09 21:35:19,"Quantum Machine Learning (QML) amalgamates quantum computing paradigms with machine learning models, providing significant prospects for solving complex problems. However, with the expansion of numerous third-party vendors in the Noisy Intermediate-Scale Quantum (NISQ) era of quantum computing, the security of QML models is of prime importance, particularly against reverse engineering, which could expose trained parameters and algorithms of the models. We assume the untrusted quantum cloud provider is an adversary having white-box access to the transpiled user-designed trained QML model during inference. Reverse engineering (RE) to extract the pre-transpiled QML circuit will enable re-transpilation and usage of the model for various hardware with completely different native gate sets and even different qubit technology. Such flexibility may not be obtained from the transpiled circuit which is tied to a particular hardware and qubit technology. The information about the number of parameters, and optimized values can allow further training of the QML model to alter the QML model, tamper with the watermark, and/or embed their own watermark or refine the model for other purposes. In this first effort to investigate the RE of QML circuits, we perform RE and compare the training accuracy of original and reverse-engineered Quantum Neural Networks (QNNs) of various sizes. We note that multi-qubit classifiers can be reverse-engineered under specific conditions with a mean error of order 1e-2 in a reasonable time. We also propose adding dummy fixed parametric gates in the QML models to increase the RE overhead for defense. For instance, adding 2 dummy qubits and 2 layers increases the overhead by ~1.76 times for a classifier with 2 qubits and 3 layers with a performance overhead of less than 9%. We note that RE is a very powerful attack model which warrants further efforts on defenses.","摘要：

本文探讨了量子机器学习(QML)模型的安全性problem，关注point是反向工程攻击RE的可能性。作者假设了不受信任的量子云服务提供商为攻击者，拥有对已经编译的、用户设计的QML模型的白盒访问权限。文章证明了RE可以成功地提取QML电路，且可以在不同的硬件和qubit技术中重新编译模型。作者还提出了添加虚拟固定的参数门控门来增加RE成本的防御方法。总之，RE是一种强大的攻击模型，需要进一步的防御措施。"
22,2407.07262v1,Differential privacy and Sublinear time are incompatible sometimes,Jeremiah Blocki; Hendrik Fichtenberger; Elena Grigorescu; Tamalika Mukherjee,2024-07-09 22:33:57,"Differential privacy and sublinear algorithms are both rapidly emerging algorithmic themes in times of big data analysis. Although recent works have shown the existence of differentially private sublinear algorithms for many problems including graph parameter estimation and clustering, little is known regarding hardness results on these algorithms. In this paper, we initiate the study of lower bounds for problems that aim for both differentially-private and sublinear-time algorithms. Our main result is the incompatibility of both the desiderata in the general case. In particular, we prove that a simple problem based on one-way marginals yields both a differentially-private algorithm, as well as a sublinear-time algorithm, but does not admit a ``strictly'' sublinear-time algorithm that is also differentially private.","本文总结：

在大数据分析中，差 privacy 和子线性算法是一些最新的演进主题。although 相关研究已经证明了大量问题，如图形参数估计和聚类分析中存在不同的私有子线性算法，但是这些问题的困难结果是较少知晓的。在本文中，我们开始研究这类算法的下限结果。我们的主要结果是指出在一般情况下，满足差隐私和子线性算法的困难性是不可达的。特别是，我们证明了基于一向 magginals 的简单问题既有差隐私算法也有子线性算法，但无法同时达到“严格”子线性算法和差隐私特征。"
