,id,title,authors,published,summary,generated_summary
0,2407.04898v1,"Nash Incentive-compatible Online Mechanism Learning via Weakly
  Differentially Private Online Learning",Joon Suk Huh; Kirthevasan Kandasamy,2024-07-06 00:02:25,"We study a multi-round mechanism design problem, where we interact with a set of agents over a sequence of rounds. We wish to design an incentive-compatible (IC) online learning scheme to maximize an application-specific objective within a given class of mechanisms, without prior knowledge of the agents' type distributions. Even if each mechanism in this class is IC in a single round, if an algorithm naively chooses from this class on each round, the entire learning process may not be IC against non-myopic buyers who appear over multiple rounds. On each round, our method randomly chooses between the recommendation of a weakly differentially private online learning algorithm (e.g., Hedge), and a commitment mechanism which penalizes non-truthful behavior. Our method is IC and achieves $O(T^{\frac{1+h}{2}})$ regret for the application-specific objective in an adversarial setting, where $h$ quantifies the long-sightedness of the agents. When compared to prior work, our approach is conceptually simpler,it applies to general mechanism design problems (beyond auctions), and its regret scales gracefully with the size of the mechanism class.", 这篇文章研究了一个多轮机制设计问题，涉及与一组参与者在多轮中的交互。研究者想设计一种 incentive-compatible (IC) 在线学习模式，以最大化应用特定目标在给定机制类中，无需事先了解参与者类型分布。即使每个机制在这个类中是单轮IC，但如果一个算法在每个轮次中从这个类中随机选择，则整个学习过程可能不会对多轮出现的非マイOPICなbuyers 保持 INCENTIVE-compatible。在每个轮次，该方法随机选择 weakly differentially private 在线学习算法（例如 Hedge）的建议和惩罚不坦 truthful 行为的承诺机制。该方法是 IC 的，并且在对手versary setting 中实现 $O(T	extsup{(1+h)/2})$，特定目标的悔心，这里 h 测量了代理人的长远视野。与先前的工作相比，该方法具有概念上更简单的优点，它适用于泛化机制设计问题（超越拍卖），并且它的悔心随机机制类的大小而缩放。
1,2407.04906v1,"Privacy or Transparency? Negotiated Smartphone Access as a Signifier of
  Trust in Romantic Relationships",Periwinkle Doerfler; Kieron Ivy Turk; Chris Geeng; Damon McCoy; Jeffrey Ackerman; Molly Dragiewicz,2024-07-06 00:52:34,"In this work, we analyze two large-scale surveys to examine how individuals think about sharing smartphone access with romantic partners as a function of trust in relationships. We find that the majority of couples have access to each others' devices, but may have explicit or implicit boundaries on how this access is to be used. Investigating these boundaries and related social norms, we find that there is little consensus about the level of smartphone access (i.e., transparency), or lack thereof (i.e., privacy) that is desirable in romantic contexts. However, there is broad agreement that the level of access should be mutual and consensual. Most individuals understand trust to be the basis of their decisions about transparency and privacy. Furthermore, we find individuals have crossed these boundaries, violating their partners' privacy and betraying their trust. We examine how, when, why, and by whom these betrayals occur. We consider the ramifications of these boundary violations in the case of intimate partner violence. Finally, we provide recommendations for design changes to enable technological enforcement of boundaries currently enforced by trust, bringing access control in line with users' sharing preferences."," 本文 analzye 两个大规模Survey,研究 Individuals sharing smartphone access与romantic partners trust level in relationships 的联系。研究发现,大多数COUPLES有对方设备的access,但可能會有EXPLICIT or IMPLICIT boundaries on how to use it。研究这些boundaries and related social norms, they find that there is little consensus on the desirable level of smartphone access (i.e., transparency) or lack thereof (i.e., privacy) in romantic contexts。然而, the majority of individuals believe that access level should be mutual and consensual。Individuals generally understand trust to be the basis of their decisions about transparency and privacy。Furthermore, they find that individuals have crossed these boundaries, violating their partners' privacy and betraying their trust。They examine the reasons for these boundary violations and consider the ramifications of these violations in the case of intimate partner violence。最后, they provide recommendations for design changes to enable technological enforcement of boundaries currently enforced by trust, aligning access control with users' sharing preferences."
2,2407.04945v1,On Differentially Private U Statistics,Kamalika Chaudhuri; Po-Ling Loh; Shourya Pandey; Purnamrita Sarkar,2024-07-06 03:27:14,"We consider the problem of privately estimating a parameter $\mathbb{E}[h(X_1,\dots,X_k)]$, where $X_1$, $X_2$, $\dots$, $X_k$ are i.i.d. data from some distribution and $h$ is a permutation-invariant function. Without privacy constraints, standard estimators are U-statistics, which commonly arise in a wide range of problems, including nonparametric signed rank tests, symmetry testing, uniformity testing, and subgraph counts in random networks, and can be shown to be minimum variance unbiased estimators under mild conditions. Despite the recent outpouring of interest in private mean estimation, privatizing U-statistics has received little attention. While existing private mean estimation algorithms can be applied to obtain confidence intervals, we show that they can lead to suboptimal private error, e.g., constant-factor inflation in the leading term, or even $\Theta(1/n)$ rather than $O(1/n^2)$ in degenerate settings. To remedy this, we propose a new thresholding-based approach using \emph{local H\'ajek projections} to reweight different subsets of the data. This leads to nearly optimal private error for non-degenerate U-statistics and a strong indication of near-optimality for degenerate U-statistics."," 本文研究了私密估计期望$\mathbb{E}[h(X_1,...X_k)]$的问题，这里$X_1, X_2, ..., X_k$是从某个分布采样的独立同 distribution数据，$h$是一个不变于置换的函数。没有私密约束的情况下，常见的估计器是U-统计量，它在许多问题中普遍出现，包括非参数双边秩检验、对称性检验、均匀性检验和随机网络中的子图计数，并且在eries mild conditions下可以被证明是最小方差无偏估计器。虽然最近对隐私中位数估计的兴趣很大，但对隐私U统计量的隐私化很少受到重视。虽然现有的隐私中位数估计算法可以用于获得置信区间，但我们表明它们可能导致子优私密误差，例如在主要项目中的常数因子膨胀，或者在冗 degeneratesettings中是O(1/n2) ，而不是Θ(1/n) 。为了解决这个问题，我们提出了一种新的基于阈值的方法，该方法使用本地H\'ajek投影重新权衡数据的不同子集。这导致了非冗U-统计量的几乎最优私密误差，并为冗U统计量提供了接近最优性的强烈暗示。"
3,2407.04959v1,Embedding Digital Signature into CSV Files Using Data Hiding,Akinori Ito,2024-07-06 04:56:22,"Open data is an important basis for open science and evidence-based policymaking. Governments of many countries disclose government-related statistics as open data. Some of these data are provided as CSV files. However, since CSV files are plain texts, we cannot ensure the integrity of a downloaded CSV file. A popular way to prove the data's integrity is a digital signature; however, it is difficult to embed a signature into a CSV file. This paper proposes a method for embedding a digital signature into a CSV file using a data hiding technique. The proposed method exploits a redundancy of the CSV format related to the use of double quotes. The experiment revealed we could embed a 512-bit signature into actual open data CSV files.", 本文提出一种在CSV文件中嵌入数字签名以证明数据完整性的方法。由于CSV文件是纯文本文件，无法确保下载的CSV文件的完整性。本文利用CSV格式的冗余，提出了一种使用数据隐蔽技术嵌入数字签名的方法，可以在实际的开放数据CSV文件中嵌入512位数字签名，并证明了该方法的有效性。这在开放科学和基于证据的政策制定中具有重要作用。
4,2407.05034v1,"GCON: Differentially Private Graph Convolutional Network via Objective
  Perturbation",Jianxin Wei; Yizheng Zhu; Xiaokui Xiao; Ergute Bao; Yin Yang; Kuntai Cai; Beng Chin Ooi,2024-07-06 09:59:56,"Graph Convolutional Networks (GCNs) are a popular machine learning model with a wide range of applications in graph analytics, including healthcare, transportation, and finance. Similar to other neural networks, a GCN may memorize parts of the training data through its model weights. Thus, when the underlying graph data contains sensitive information such as interpersonal relationships, a GCN trained without privacy-protection measures could be exploited to extract private data, leading to potential violations of privacy regulations such as GDPR. To defend against such attacks, a promising approach is to train the GCN with differential privacy (DP), which is a rigorous framework that provides strong privacy protection by injecting random noise into the trained model weights. However, training a large graph neural network under DP is a highly challenging task. Existing solutions either introduce random perturbations in the graph topology, which leads to severe distortions of the network's message passing, or inject randomness into each neighborhood aggregation operation, which leads to a high noise scale when the GCN performs multiple levels of aggregations. Motivated by this, we propose GCON, a novel and effective solution for training GCNs with edge differential privacy. The main idea is to (i) convert the GCN training process into a convex optimization problem, and then (ii) apply the classic idea of perturbing the objective function to satisfy DP. Extensive experiments using multiple benchmark datasets demonstrate GCON's consistent and superior performance over existing solutions in a wide variety of settings.", 该文章介绍了Graph Convolutional Networks (GCNs) 作为一种有广泛应用的机器学习模型，如在健康、交通和金融等领域的图 analytics 中。同样的，GCN 类似于其他神经网络可能通过其模型权重记忆训练数据的一部分。因此，当基础图数据包含敏感信息（如人际关系）时，未经隐私保护措施的训练 GCNS 可能被利用以提取私人数据，从而可能违反隐私法规，例如GDPR。为了防御这些攻击，一个有前途的方法是使用 differential privacy (DP) 训练 GCN，DP 是一个严格的框架，通过在训练模型权重中注入随机噪声提供强大的隐私保护。但是，在 DP 下训练大型图神经网络是一项具有挑战性的任务。现有解决方案要么在图拓扑上引入随机扰动，这导致网络消息传递被严重脱标，要么在每个邻域聚合操作中注入随机性，这导致GCN 执行多级聚合时高噪音水平。受此启发，我们提出了 GCON，一种训练 GCNs with edge differential privacy 的新颖有效解决方案。主要思想是将 GCN 训练过程转换为凸优化问题，然后应用经典思想：通过满足 DP 来扰动目标函数。大量实验使用多个基准数据集表明，GCON 在各种场景中始终表现优于现有解决方案。
5,2407.05064v1,Reverse Engineered MiniFS File System,Dmitrii Belimov; Evgenii Vinogradov,2024-07-06 12:49:37,"In an era where digital connectivity is increasingly foundational to daily life, the security of Wi-Fi Access Points (APs) is a critical concern. This paper addresses the vulnerabilities inherent in Wi-Fi APs, with a particular focus on those using proprietary file systems like MiniFS found in TP-Link's AC1900 WiFi router. Through reverse engineering, we unravel the structure and operation of MiniFS, marking a significant advancement in our understanding of this previously opaque file system. Our investigation reveals not only the architecture of MiniFS but also identifies several private keys and underscores a concerning lack of cryptographic protection. These findings point to broader security vulnerabilities, emphasizing the risks of security-by-obscurity practices in an interconnected environment. Our contributions are twofold: firstly, based, on the file system structure, we develop a methodology for the extraction and analysis of MiniFS, facilitating the identification and mitigation of potential vulnerabilities. Secondly, our work lays the groundwork for further research into WiFi APs' security, particularly those running on similar proprietary systems. By highlighting the critical need for transparency and community engagement in firmware analysis, this study contributes to the development of more secure network devices, thus enhancing the overall security posture of digital infrastructures.", 本文主要讨论当下数字化连接日益基本化的背景下，Wi-Fi接入点（AP）的安全问题，尤其是使用专有文件系统（例如MiniFS，在TP-Link的AC1900无线路由器中使用）的AP。通过反向工程，本文揭开MiniFS的结构和操作方式，这是对此前被视为不透明的文件系统的一个重大进展。研究发现，MiniFS不仅揭示了其架构，还确认了 Several private keys的存在，并指出了加密保护的缺乏，这表明了更广泛的安全漏洞，并强调了在相互连接的环境中采用安全隐蔽策略的风险。本研究的贡献有二点：首先，根据文件系统结构，我们开发了一种用于提取和分析MiniFS的方法，有助于识别和缓解潜在漏洞。其次，本文为研究WiFi APs的安全（尤其是在类似专有系统上运行的AP）奠定了基础。通过强调透明度和社区参与在固件分析中的关键性，本研究有助于开发更安全的网络设备，从而提高数字基础设施的整体安全态势。
6,2407.05067v1,Smooth Sensitivity Revisited: Towards Optimality,Richard Hladík; Jakub Tětek,2024-07-06 13:09:36,"Smooth sensitivity is one of the most commonly used techniques for designing practical differentially private mechanisms. In this approach, one computes the smooth sensitivity of a given query $q$ on the given input $D$ and releases $q(D)$ with noise added proportional to this smooth sensitivity. One question remains: what distribution should we pick the noise from?   In this paper, we give a new class of distributions suitable for the use with smooth sensitivity, which we name the PolyPlace distribution. This distribution improves upon the state-of-the-art Student's T distribution in terms of standard deviation by arbitrarily large factors, depending on a ""smoothness parameter"" $\gamma$, which one has to set in the smooth sensitivity framework. Moreover, our distribution is defined for a wider range of parameter $\gamma$, which can lead to significantly better performance.   Moreover, we prove that the PolyPlace distribution converges for $\gamma \rightarrow 0$ to the Laplace distribution and so does its variance. This means that the Laplace mechanism is a limit special case of the PolyPlace mechanism. This implies that out mechanism is in a certain sense optimal for $\gamma \to 0$."," 本文介绍了一种名为“平滑敏感度”（Smooth Sensitivity）的常用技术，用于设计实用的差分私有机制。在这种方法中，计算查询 $q$ 在给定输入 $D$ 上的平滑敏感度，然后添加成比例的噪声来发布 $q(D)$。本文的主要贡献在于提出了一类与平滑敏感度兼容的新分布，称为 PolyPlace 分布。与使用 Student's T 分布相比，PolyPlace 分布在“平滑参数” $\gamma$ 的帮助下，可以通过任意大的因素改善标准偏差。此外，PolyPlace 分布在参数 $\gamma$ 的范围更广，这可以导致显著的性能改善。

此外，本文还证明了当 $\gamma$ 趋向于 0 时，PolyPlace 分布将收敛于Λ片 distribution，它的方差也将收敛到该值。这意味着Λ片机制是 PolyPlace 机制的一个极限特例，这表明我们的机制在 $\gamma$ 趋向于 0 时在某种意义上是最优的。"
7,2407.05112v1,"Releasing Malevolence from Benevolence: The Menace of Benign Data on
  Machine Unlearning",Binhao Ma; Tianhang Zheng; Hongsheng Hu; Di Wang; Shuo Wang; Zhongjie Ba; Zhan Qin; Kui Ren,2024-07-06 15:42:28,"Machine learning models trained on vast amounts of real or synthetic data often achieve outstanding predictive performance across various domains. However, this utility comes with increasing concerns about privacy, as the training data may include sensitive information. To address these concerns, machine unlearning has been proposed to erase specific data samples from models. While some unlearning techniques efficiently remove data at low costs, recent research highlights vulnerabilities where malicious users could request unlearning on manipulated data to compromise the model. Despite these attacks' effectiveness, perturbed data differs from original training data, failing hash verification. Existing attacks on machine unlearning also suffer from practical limitations and require substantial additional knowledge and resources. To fill the gaps in current unlearning attacks, we introduce the Unlearning Usability Attack. This model-agnostic, unlearning-agnostic, and budget-friendly attack distills data distribution information into a small set of benign data. These data are identified as benign by automatic poisoning detection tools due to their positive impact on model training. While benign for machine learning, unlearning these data significantly degrades model information. Our evaluation demonstrates that unlearning this benign data, comprising no more than 1% of the total training data, can reduce model accuracy by up to 50%. Furthermore, our findings show that well-prepared benign data poses challenges for recent unlearning techniques, as erasing these synthetic instances demands higher resources than regular data. These insights underscore the need for future research to reconsider ""data poisoning"" in the context of machine unlearning.", 本文介绍了Machine unlearning（机器学习未学习），这是一种机器学习模型删除特定数据样本的技术，可以解决数据中敏感信息带来的隐私关切。同时，文章指出，现有的未学习技术在删除数据时可能存在漏洞，可能会被恶意用户利用，进而破坏模型。该文章进一步提出了一种名为Unlearning Usability Attack的攻击方法，该攻击方法通过一个小的数据集来总结数据分布信息，这些数据被判定为无害的，因为它们对模型训练有正面的影响。但是，未学习这些无害的数据将会对模型造成严重的负面影响。这种攻击方法易于实现，且与机器学习模型和未学习技术无关，这使得它更容易被应用到现有的未学习技术中，进而提高其安全性。
8,2407.05182v1,"A Novel Bifurcation Method for Observation Perturbation Attacks on
  Reinforcement Learning Agents: Load Altering Attacks on a Cyber Physical
  Power System",Kiernan Broda-Milian; Ranwa Al-Mallah; Hanane Dagdougui,2024-07-06 20:55:24,"Components of cyber physical systems, which affect real-world processes, are often exposed to the internet. Replacing conventional control methods with Deep Reinforcement Learning (DRL) in energy systems is an active area of research, as these systems become increasingly complex with the advent of renewable energy sources and the desire to improve their efficiency. Artificial Neural Networks (ANN) are vulnerable to specific perturbations of their inputs or features, called adversarial examples. These perturbations are difficult to detect when properly regularized, but have significant effects on the ANN's output. Because DRL uses ANN to map optimal actions to observations, they are similarly vulnerable to adversarial examples. This work proposes a novel attack technique for continuous control using Group Difference Logits loss with a bifurcation layer. By combining aspects of targeted and untargeted attacks, the attack significantly increases the impact compared to an untargeted attack, with drastically smaller distortions than an optimally targeted attack. We demonstrate the impacts of powerful gradient-based attacks in a realistic smart energy environment, show how the impacts change with different DRL agents and training procedures, and use statistical and time-series analysis to evaluate attacks' stealth. The results show that adversarial attacks can have significant impacts on DRL controllers, and constraining an attack's perturbations makes it difficult to detect. However, certain DRL architectures are far more robust, and robust training methods can further reduce the impact.", 本文研究网络energency系统中采用深度强化学习(DRL)的控制方法的安全问题。DRL使用人工神经网络(ANN)将最优动作映射到观察值，ANN易受特定输入或特征扰动的影响，称为对抗性示例，这些扰动难以检测但会对ANN输出产生重大影响。因此，DRL也易受到对抗性示例的影响。本文提出一种针对连续控制的新攻击技术，即使用Group Difference Logits loss与分支层。该攻击组合了目标攻击和非目标攻击的方面，使攻击的影响显著提高，而扰动则比最优目标攻击要小得多。实验表明，对DLR控制器的强大梯度攻击在真实的智能能量环境中可能会产生重大影响，攻击对不同DRL代理和训练过程的影响也会有所不同，并使用统计和时序分析评估攻击的隐蔽性。结果表明，对DLR控制器的对抗性攻击可能会产生显著影响，并且约束攻击的扰动使其难以检测。但是，某些DRL架构更加稳健，并且可以通过稳健的训练方法进一步降低攻击的 impact。
9,2407.05194v1,"LLMCloudHunter: Harnessing LLMs for Automated Extraction of Detection
  Rules from Cloud-Based CTI",Yuval Schwartz; Lavi Benshimol; Dudu Mimran; Yuval Elovici; Asaf Shabtai,2024-07-06 21:43:35,"As the number and sophistication of cyber attacks have increased, threat hunting has become a critical aspect of active security, enabling proactive detection and mitigation of threats before they cause significant harm. Open-source cyber threat intelligence (OS-CTI) is a valuable resource for threat hunters, however, it often comes in unstructured formats that require further manual analysis. Previous studies aimed at automating OSCTI analysis are limited since (1) they failed to provide actionable outputs, (2) they did not take advantage of images present in OSCTI sources, and (3) they focused on on-premises environments, overlooking the growing importance of cloud environments. To address these gaps, we propose LLMCloudHunter, a novel framework that leverages large language models (LLMs) to automatically generate generic-signature detection rule candidates from textual and visual OSCTI data. We evaluated the quality of the rules generated by the proposed framework using 12 annotated real-world cloud threat reports. The results show that our framework achieved a precision of 92% and recall of 98% for the task of accurately extracting API calls made by the threat actor and a precision of 99% with a recall of 98% for IoCs. Additionally, 99.18% of the generated detection rule candidates were successfully compiled and converted into Splunk queries."," 本文提出了 LLMCloudHunter，一个新的框架，它使用大语言模型（LLMs）自动从文本和视觉的开源网络威胁情报（OS-CTI）数据中生成通用签名检测规则候选项。当网络攻击的数量和复杂性增加时，威胁狩猎在主动安全中变得至关重要，可以有效地检测和减轻威胁，避免造成重大损害。然而，OS-CTI frequently comes in unstructured formats that require further manual analysis. Previous studies have limitations in automating OSCTI analysis, such as failing to provide actionable outputs, not utilizing images in OSCTI sources, and focusing only on on-premises environments. The proposed framework addresses these gaps by generating high-quality detection rules from textual and visual OSCTI data, as demonstrated by an evaluation using 12 annotated real-world cloud threat reports. LLMCloudHunter achieved a precision of 92% and recall of 98% in accurately extracting API calls made by the threat actor and a precision of 99% with a recall of 98% for IoCs. Furthermore, 99.18% of the generated detection rule candidates were successfully compiled and converted into Splunk queries."
