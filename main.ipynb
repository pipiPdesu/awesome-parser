{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loader.daily_loader import load_daily_paper\n",
    "from chater.chat import mapreduce\n",
    "import os\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "import faiss\n",
    "import json\n",
    "\n",
    "from vec_generate.arxiv_generate import arxiv_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#初始化部分\n",
    "os.environ[\"NVIDIA_API_KEY\"] = \"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\"\n",
    "model = ChatNVIDIA(model=\"ai-llama3-8b\")#.bind(max_tokens=4096)\n",
    "embedder = NVIDIAEmbeddings(model=\"ai-embed-qa-4\", truncate=\"END\")\n",
    "cache = {}\n",
    "try:\n",
    "    cache = json.load(open(\"cache.json\"))  # 读取用户有过哪些向量库   里面存的应该是tuple (vectorstore_path, initial_msg)\n",
    "except:\n",
    "    cache = {}\n",
    "    json.dump(cache, open(\"cache.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedder.embed_query(\"test\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里先不给用户造自己的库的功能 只有daily chat和 awesome chat 两个功能  本地有个csv表格记录已经做了哪些库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def daily_chat(date, cat, model, embedder, cache={}, verbose=False):\n",
    "\n",
    "    '''\n",
    "    接受日期 类别 模型和cache, 返回一个向量库路径和一个开场词\n",
    "    如果用户指定verbose为True, 则当天的arxiv表格会保存在该库本地\n",
    "    '''\n",
    "\n",
    "    if date+cat in cache:\n",
    "        return cache[date+cat]\n",
    "    \n",
    "    else:\n",
    "        daily = load_daily_paper(date)\n",
    "\n",
    "        if(len(daily) == 0):\n",
    "            print(\"No papers found for this date and category, please retry\")\n",
    "            return None, None\n",
    "\n",
    "        summary, detail = mapreduce(model, daily)\n",
    "        if verbose:\n",
    "            daily.to_csv(f\"{date}_{cat}.csv\")  # 保存当天的arxiv表格\n",
    "        \n",
    "        path_name = 'local_embed/'+date+cat\n",
    "        arxiv_generate(detail['id'], embedder, path_name)\n",
    "        \n",
    "        cache[date+cat] = (path_name, summary)\n",
    "        json.dump(cache, open(\"cache.json\", \"w\"))\n",
    "\n",
    "        return path_name, summary\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loader.awesome_loader import load_from_awesome\n",
    "def awesome_chat(md_path, embedder, cache={}):\n",
    "    '''\n",
    "    接受md路径和cache, 返回一个向量库路径和一个开场词\n",
    "    '''\n",
    "    if md_path in cache:\n",
    "        return cache[md_path]\n",
    "    else:\n",
    "        with open(md_path, 'r') as f:\n",
    "            text = f.read()\n",
    "            \n",
    "        path_name = 'local_embed/'+md_path.split('.')[0]\n",
    "        \n",
    "        paper_list = load_from_awesome(text)\n",
    "\n",
    "        msg = (\n",
    "    \"你好，我是awesome-parser， 一个帮助用户解析awesome-list的工具。\"\n",
    "    f\"我从您提供的文档{md_path}中解析到了{len(paper_list)}篇paper，我可以帮助您吗\")\n",
    "\n",
    "        arxiv_generate(paper_list, embedder, path_name)\n",
    "        cache[md_path] = (path_name, msg)\n",
    "        json.dump(cache, open(\"cache.json\", \"w\"))\n",
    "\n",
    "        return path_name, msg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "path, init_msg = daily_chat(\"20240707\", \"cs.CR\", model, embedder, cache, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Documents\n",
      "Cleaning Documents\n",
      "Chunking Documents\n",
      "Creating Vectorstores\n"
     ]
    }
   ],
   "source": [
    "path, init_msg = awesome_chat(\"prompt_injection.md\", embedder, cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local_embed/prompt_injection 你好，我是awesome-parser， 一个帮助用户解析awesome-list的工具。我从您提供的文档prompt_injection.md中解析到了16篇paper，我可以帮助您吗\n"
     ]
    }
   ],
   "source": [
    "print(path, init_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecstores = [FAISS.load_local(folder_path=path, embeddings=embedder,allow_dangerous_deserialization=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed aggregate docstore with 3470 chunks\n"
     ]
    }
   ],
   "source": [
    "from vec_generate.arxiv_generate import aggregate_vstores\n",
    "\n",
    "docstore = aggregate_vstores(vecstores, embedder)\n",
    "print(f\"Constructed aggregate docstore with {len(docstore.docstore._dict)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vec_generate.arxiv_generate import default_FAISS\n",
    "convstore = default_FAISS(embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from functools import partial\n",
    "from langchain.document_transformers import LongContextReorder\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables.passthrough import RunnableAssign\n",
    "\n",
    "    \n",
    "def save_memory_and_get_output(d, vstore):\n",
    "    \"\"\"Accepts 'input'/'output' dictionary and saves to convstore\"\"\"\n",
    "    vstore.add_texts([\n",
    "        f\"User previously responded with {d.get('input')}\",\n",
    "        f\"Agent previously responded with {d.get('output')}\"\n",
    "    ])\n",
    "    return d.get('output')\n",
    "\n",
    "initial_msg = init_msg\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([(\"system\",\n",
    "    \"You are a document chatbot. Help the user as they ask questions about documents.\"\n",
    "    \" User messaged just asked: {input}\\n\\n\"\n",
    "    \" From this, we have retrieved the following potentially-useful info: \"\n",
    "    \" Conversation History Retrieval:\\n{history}\\n\\n\"\n",
    "    \" Document Retrieval:\\n{context}\\n\\n\"\n",
    "    \" (Answer only from retrieval. Only cite sources that are used. Make your response conversational.Reply must more than 100 words)\"\n",
    "), ('user', '{input}')])\n",
    "\n",
    "## Utility Runnables/Methods\n",
    "def RPrint(preface=\"\"):\n",
    "    \"\"\"Simple passthrough \"prints, then returns\" chain\"\"\"\n",
    "    def print_and_return(x, preface):\n",
    "        print(f\"{preface}{x}\")\n",
    "        return x\n",
    "    return RunnableLambda(partial(print_and_return, preface=preface))\n",
    "\n",
    "def docs2str(docs, title=\"Document\"):\n",
    "    \"\"\"Useful utility for making chunks into context string. Optional, but useful\"\"\"\n",
    "    out_str = \"\"\n",
    "    for doc in docs:\n",
    "        doc_name = getattr(doc, 'metadata', {}).get('Title', title)\n",
    "        if doc_name:\n",
    "            out_str += f\"[Quote from {doc_name}] \"\n",
    "        out_str += getattr(doc, 'page_content', str(doc)) + \"\\n\"\n",
    "    return out_str\n",
    "\n",
    "## 将较长的文档重新排序到输出文本的中心， RunnableLambda在链中运行无参自定义函数 ，长上下文重排序（LongContextReorder）\n",
    "long_reorder = RunnableLambda(LongContextReorder().transform_documents)\n",
    "\n",
    "retrieval_chain = (\n",
    "    {'input' : (lambda x: x)}\n",
    "    | RunnableAssign({'history' : itemgetter('input') | convstore.as_retriever() | long_reorder | docs2str})\n",
    "    | RunnableAssign({'context' : itemgetter('input') | docstore.as_retriever()  | long_reorder | docs2str})\n",
    "    | RPrint()\n",
    ")\n",
    "stream_chain = chat_prompt | model | StrOutputParser()\n",
    "\n",
    "def chat_gen(message, history=[], return_buffer=True):\n",
    "    buffer = \"\"\n",
    "    ##首先根据输入的消息进行检索\n",
    "    retrieval = retrieval_chain.invoke(message)\n",
    "    line_buffer = \"\"\n",
    "\n",
    "    ## 然后流式传输stream_chain的结果\n",
    "    for token in stream_chain.stream(retrieval):\n",
    "        buffer += token\n",
    "        ## 优化信息打印的格式\n",
    "        if not return_buffer:\n",
    "            line_buffer += token\n",
    "            if \"\\n\" in line_buffer:\n",
    "                line_buffer = \"\"\n",
    "            if ((len(line_buffer)>84 and token and token[0] == \" \") or len(line_buffer)>100):\n",
    "                line_buffer = \"\"\n",
    "                yield \"\\n\"\n",
    "                token = \"  \" + token.lstrip()\n",
    "        yield buffer if return_buffer else token\n",
    "\n",
    "    ##最后将聊天内容保存到对话内存缓冲区中\n",
    "    save_memory_and_get_output({'input':  message, 'output': buffer}, convstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://0.0.0.0:5000\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:5000/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': '什么是prompt注入攻击', 'history': '', 'context': '[Quote from Prompt Injection Attacks in Defended Systems] competition aims to foster innovation and collaboration in the\\nfield of LLM security.\\nA. Defense Prompt\\nThe defense prompt is appended to the general system\\nprompt and the secret-carrying sentence in the system prompt\\n[Quote from Prompt Injection Attacks in Defended Systems] to explain when a question does not make sense or is not\\nfactually coherent, and to avoid sharing false information when\\nthe answer is unknown. The defense prompt is guaranteed to be\\nthe first message in any chat with this defense and is restricted\\n[Quote from Prompt Injection Attacks in Defended Systems] use the fields Age, Value, Name, and already ask to fill the\\nValue field with \"your value.\"\\nThe following are several of the most successful prompts in\\nEnglish that were used:\\n• An attack using a json object\\n[Quote from Prompt Injection Attacks in Defended Systems] or the first message of the chat. The prompt ensures that the\\nassistant provides helpful, respectful, and honest responses\\nwhile avoiding harmful, unethical, racist, sexist, toxic, danger-\\nous, or illegal content. The prompt encourages the assistant\\n'}\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "chatbot = gr.Chatbot(value = [[None, initial_msg]])\n",
    "demo = gr.ChatInterface(chat_gen, chatbot=chatbot).queue()\n",
    "\n",
    "try:\n",
    "    demo.launch(debug=True, share=False, show_api=False, server_port=5000, server_name=\"0.0.0.0\")\n",
    "    demo.close()\n",
    "except Exception as e:\n",
    "    demo.close()\n",
    "    print(e)\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmdeploy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
