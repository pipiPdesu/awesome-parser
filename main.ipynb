{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loader.daily_loader import load_daily_paper\n",
    "from chater.chat import mapreduce\n",
    "import os\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "import faiss\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#初始化部分\n",
    "os.environ[\"NVIDIA_API_KEY\"] = \"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\"\n",
    "model = ChatNVIDIA(model=\"ai-llama3-8b\")#.bind(max_tokens=4096)\n",
    "embedder = NVIDIAEmbeddings(model=\"ai-embed-qa-4\")\n",
    "cache = {}\n",
    "try:\n",
    "    cache = json.load(open(\"cache.json\"))  # 读取用户有过哪些向量库   里面存的应该是tuple (vectorstore_path, initial_msg)\n",
    "except:\n",
    "    cache = {}\n",
    "    json.dump(cache, open(\"cache.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里先不给用户造自己的库的功能 只有daily chat和 awesome chat 两个功能  本地有个csv表格记录已经做了哪些库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vec_generate.arxiv_generate import arxiv_generate\n",
    "\n",
    "def daily_chat(date, cat, model, embedder, cache={}, verbose=False):\n",
    "\n",
    "    '''\n",
    "    接受日期 类别 模型和cache, 返回一个向量库路径和一个开场词\n",
    "    如果用户指定verbose为True, 则当天的arxiv表格会保存在该库本地\n",
    "    '''\n",
    "\n",
    "    if date+cat in cache:\n",
    "        return cache[date+cat]\n",
    "    \n",
    "    else:\n",
    "        daily = load_daily_paper(date)\n",
    "\n",
    "        if(len(daily) == 0):\n",
    "            print(\"No papers found for this date and category, please retry\")\n",
    "            return None, None\n",
    "\n",
    "        summary, detail = mapreduce(model, daily)\n",
    "        if verbose:\n",
    "            daily.to_csv(f\"{date}_{cat}.csv\")  # 保存当天的arxiv表格\n",
    "        \n",
    "        path_name = 'local_embed/'+date+cat\n",
    "        arxiv_generate(detail['id'][:2], embedder, path_name)\n",
    "        \n",
    "        cache[date+cat] = (path_name, summary)\n",
    "        json.dump(cache, open(\"cache.json\", \"w\"))\n",
    "\n",
    "        return path_name, summary\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "path, init_msg = daily_chat(\"20240707\", \"cs.CR\", model, embedder, cache, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecstores = [FAISS.load_local(folder_path=path, embeddings=embedder,allow_dangerous_deserialization=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed aggregate docstore with 223 chunks\n"
     ]
    }
   ],
   "source": [
    "from vec_generate.arxiv_generate import aggregate_vstores\n",
    "\n",
    "docstore = aggregate_vstores(vecstores, embedder)\n",
    "print(f\"Constructed aggregate docstore with {len(docstore.docstore._dict)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vec_generate.arxiv_generate import default_FAISS\n",
    "convstore = default_FAISS(embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from functools import partial\n",
    "from langchain.document_transformers import LongContextReorder\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables.passthrough import RunnableAssign\n",
    "\n",
    "    \n",
    "def save_memory_and_get_output(d, vstore):\n",
    "    \"\"\"Accepts 'input'/'output' dictionary and saves to convstore\"\"\"\n",
    "    vstore.add_texts([\n",
    "        f\"User previously responded with {d.get('input')}\",\n",
    "        f\"Agent previously responded with {d.get('output')}\"\n",
    "    ])\n",
    "    return d.get('output')\n",
    "\n",
    "initial_msg = init_msg\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([(\"system\",\n",
    "    \"You are a document chatbot. Help the user as they ask questions about documents.\"\n",
    "    \" User messaged just asked: {input}\\n\\n\"\n",
    "    \" From this, we have retrieved the following potentially-useful info: \"\n",
    "    \" Conversation History Retrieval:\\n{history}\\n\\n\"\n",
    "    \" Document Retrieval:\\n{context}\\n\\n\"\n",
    "    \" (Answer only from retrieval. Only cite sources that are used. Make your response conversational.Reply must more than 100 words)\"\n",
    "), ('user', '{input}')])\n",
    "\n",
    "## Utility Runnables/Methods\n",
    "def RPrint(preface=\"\"):\n",
    "    \"\"\"Simple passthrough \"prints, then returns\" chain\"\"\"\n",
    "    def print_and_return(x, preface):\n",
    "        print(f\"{preface}{x}\")\n",
    "        return x\n",
    "    return RunnableLambda(partial(print_and_return, preface=preface))\n",
    "\n",
    "def docs2str(docs, title=\"Document\"):\n",
    "    \"\"\"Useful utility for making chunks into context string. Optional, but useful\"\"\"\n",
    "    out_str = \"\"\n",
    "    for doc in docs:\n",
    "        doc_name = getattr(doc, 'metadata', {}).get('Title', title)\n",
    "        if doc_name:\n",
    "            out_str += f\"[Quote from {doc_name}] \"\n",
    "        out_str += getattr(doc, 'page_content', str(doc)) + \"\\n\"\n",
    "    return out_str\n",
    "\n",
    "## 将较长的文档重新排序到输出文本的中心， RunnableLambda在链中运行无参自定义函数 ，长上下文重排序（LongContextReorder）\n",
    "long_reorder = RunnableLambda(LongContextReorder().transform_documents)\n",
    "\n",
    "retrieval_chain = (\n",
    "    {'input' : (lambda x: x)}\n",
    "    | RunnableAssign({'history' : itemgetter('input') | convstore.as_retriever() | long_reorder | docs2str})\n",
    "    | RunnableAssign({'context' : itemgetter('input') | docstore.as_retriever()  | long_reorder | docs2str})\n",
    "    | RPrint()\n",
    ")\n",
    "stream_chain = chat_prompt | model | StrOutputParser()\n",
    "\n",
    "def chat_gen(message, history=[], return_buffer=True):\n",
    "    buffer = \"\"\n",
    "    ##首先根据输入的消息进行检索\n",
    "    retrieval = retrieval_chain.invoke(message)\n",
    "    line_buffer = \"\"\n",
    "\n",
    "    ## 然后流式传输stream_chain的结果\n",
    "    for token in stream_chain.stream(retrieval):\n",
    "        buffer += token\n",
    "        ## 优化信息打印的格式\n",
    "        if not return_buffer:\n",
    "            line_buffer += token\n",
    "            if \"\\n\" in line_buffer:\n",
    "                line_buffer = \"\"\n",
    "            if ((len(line_buffer)>84 and token and token[0] == \" \") or len(line_buffer)>100):\n",
    "                line_buffer = \"\"\n",
    "                yield \"\\n\"\n",
    "                token = \"  \" + token.lstrip()\n",
    "        yield buffer if return_buffer else token\n",
    "\n",
    "    ##最后将聊天内容保存到对话内存缓冲区中\n",
    "    save_memory_and_get_output({'input':  message, 'output': buffer}, convstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "chatbot = gr.Chatbot(value = [[None, initial_msg]])\n",
    "demo = gr.ChatInterface(chat_gen, chatbot=chatbot).queue()\n",
    "\n",
    "try:\n",
    "    demo.launch(debug=True, share=False, show_api=False, server_port=5000, server_name=\"0.0.0.0\")\n",
    "    demo.close()\n",
    "except Exception as e:\n",
    "    demo.close()\n",
    "    print(e)\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmdeploy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
