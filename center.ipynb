{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xing\\.conda\\envs\\lmdeploy\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from loader.daily_loader import load_daily_paper\n",
    "from chater.chat import mapreduce\n",
    "import os\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain.document_loaders import ArxivLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from faiss import IndexFlatL2\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from operator import itemgetter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"NVIDIA_API_KEY\"] = \"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\"\n",
    "model = ChatNVIDIA(model=\"ai-mixtral-8x7b-instruct\").bind(max_tokens=4096)\n",
    "embedder = NVIDIAEmbeddings(model=\"ai-embed-qa-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily = load_daily_paper('20240705')\n",
    "summary, detail = mapreduce(model, daily)\n",
    "# 这里能不能并行造向量库啊\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    ArxivLoader(query = detail['id'][i]).load() for i in range(len(detail))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "## Cut the paper short if references is included.\n",
    "## This is a standard string in papers.\n",
    "for doc in docs:\n",
    "    content = json.dumps(doc[0].page_content)\n",
    "    if \"References\" in content:\n",
    "        doc[0].page_content = content[:content.index(\"References\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \";\", \",\", \" \"],\n",
    ")\n",
    "## Split the documents and also filter out stubs (overly short chunks)\n",
    "print(\"Chunking Documents\")\n",
    "docs_chunks = [text_splitter.split_documents(doc) for doc in docs]\n",
    "docs_chunks = [[c for c in dchunks if len(c.page_content) > 200] for dchunks in docs_chunks]\n",
    "#注意这里的200会有误伤  比如https://arxiv.org/abs/2407.04757 这篇裁完基本上没东西了\n",
    "\n",
    "## Make some custom Chunks to give big-picture details\n",
    "doc_string = \"Available Documents:\"\n",
    "doc_metadata = []\n",
    "for chunks in docs_chunks:\n",
    "    metadata = getattr(chunks[0], 'metadata', {})\n",
    "    doc_string += \"\\n - \" + metadata.get('Title')\n",
    "    doc_metadata += [str(metadata)]\n",
    "\n",
    "extra_chunks = [doc_string] + doc_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Printing out some summary information for reference\n",
    "print(doc_string, '\\n')\n",
    "for i, chunks in enumerate(docs_chunks):\n",
    "    print(f\"Document {i}\")\n",
    "    print(f\" - # Chunks: {len(chunks)}\")\n",
    "    print(f\" - Metadata: \")\n",
    "    print(chunks[0].metadata)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Constructing Vector Stores\")\n",
    "vecstores = [FAISS.from_texts(extra_chunks, embedder)]\n",
    "vecstores += [FAISS.from_documents(doc_chunks, embedder) for doc_chunks in docs_chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecstores = [FAISS.load_local(folder_path=\"E:/sona/awesome-parser/docstore_index\", embeddings=embedder,allow_dangerous_deserialization=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed aggregate docstore with 19 chunks\n"
     ]
    }
   ],
   "source": [
    "embed_dims = len(embedder.embed_query(\"test\"))\n",
    "def default_FAISS():\n",
    "    '''Useful utility for making an empty FAISS vectorstore'''\n",
    "    return FAISS(\n",
    "        embedding_function=embedder,\n",
    "        index=IndexFlatL2(embed_dims),\n",
    "        docstore=InMemoryDocstore(),\n",
    "        index_to_docstore_id={},\n",
    "        normalize_L2=False\n",
    "    )\n",
    "\n",
    "def aggregate_vstores(vectorstores):\n",
    "    ## Initialize an empty FAISS Index and merge others into it\n",
    "    ## We'll use default_faiss for simplicity, though it's tied to your embedder by reference\n",
    "    agg_vstore = default_FAISS()\n",
    "    for vstore in vectorstores:\n",
    "        agg_vstore.merge_from(vstore)\n",
    "    return agg_vstore\n",
    "\n",
    "## Unintuitive optimization; merge_from seems to optimize constituent vector stores away\n",
    "docstore = aggregate_vstores(vecstores)\n",
    "\n",
    "print(f\"Constructed aggregate docstore with {len(docstore.docstore._dict)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docstore.save_local(\"docstore_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "convstore = default_FAISS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from functools import partial\n",
    "from langchain.document_transformers import LongContextReorder\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables.passthrough import RunnableAssign\n",
    "\n",
    "    \n",
    "def save_memory_and_get_output(d, vstore):\n",
    "    \"\"\"Accepts 'input'/'output' dictionary and saves to convstore\"\"\"\n",
    "    vstore.add_texts([\n",
    "        f\"User previously responded with {d.get('input')}\",\n",
    "        f\"Agent previously responded with {d.get('output')}\"\n",
    "    ])\n",
    "    return d.get('output')\n",
    "\n",
    "initial_msg = (\n",
    "    \"Hello! I am a document chat agent here to help the user!\"\n",
    "    f\" I have access to the following documents: \\n\\nHow can I help you?\"\n",
    ")\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([(\"system\",\n",
    "    \"You are a document chatbot. Help the user as they ask questions about documents.\"\n",
    "    \" User messaged just asked: {input}\\n\\n\"\n",
    "    \" From this, we have retrieved the following potentially-useful info: \"\n",
    "    \" Conversation History Retrieval:\\n{history}\\n\\n\"\n",
    "    \" Document Retrieval:\\n{context}\\n\\n\"\n",
    "    \" (Answer only from retrieval. Only cite sources that are used. Make your response conversational.Reply must more than 100 words)\"\n",
    "), ('user', '{input}')])\n",
    "\n",
    "## Utility Runnables/Methods\n",
    "def RPrint(preface=\"\"):\n",
    "    \"\"\"Simple passthrough \"prints, then returns\" chain\"\"\"\n",
    "    def print_and_return(x, preface):\n",
    "        print(f\"{preface}{x}\")\n",
    "        return x\n",
    "    return RunnableLambda(partial(print_and_return, preface=preface))\n",
    "\n",
    "def docs2str(docs, title=\"Document\"):\n",
    "    \"\"\"Useful utility for making chunks into context string. Optional, but useful\"\"\"\n",
    "    out_str = \"\"\n",
    "    for doc in docs:\n",
    "        doc_name = getattr(doc, 'metadata', {}).get('Title', title)\n",
    "        if doc_name:\n",
    "            out_str += f\"[Quote from {doc_name}] \"\n",
    "        out_str += getattr(doc, 'page_content', str(doc)) + \"\\n\"\n",
    "    return out_str\n",
    "\n",
    "## 将较长的文档重新排序到输出文本的中心， RunnableLambda在链中运行无参自定义函数 ，长上下文重排序（LongContextReorder）\n",
    "long_reorder = RunnableLambda(LongContextReorder().transform_documents)\n",
    "\n",
    "retrieval_chain = (\n",
    "    {'input' : (lambda x: x)}\n",
    "    | RunnableAssign({'history' : itemgetter('input') | convstore.as_retriever() | long_reorder | docs2str})\n",
    "    | RunnableAssign({'context' : itemgetter('input') | docstore.as_retriever()  | long_reorder | docs2str})\n",
    "    | RPrint()\n",
    ")\n",
    "stream_chain = chat_prompt | model | StrOutputParser()\n",
    "\n",
    "def chat_gen(message, history=[], return_buffer=True):\n",
    "    buffer = \"\"\n",
    "    ##首先根据输入的消息进行检索\n",
    "    retrieval = retrieval_chain.invoke(message)\n",
    "    line_buffer = \"\"\n",
    "\n",
    "    ## 然后流式传输stream_chain的结果\n",
    "    for token in stream_chain.stream(retrieval):\n",
    "        buffer += token\n",
    "        ## 优化信息打印的格式\n",
    "        if not return_buffer:\n",
    "            line_buffer += token\n",
    "            if \"\\n\" in line_buffer:\n",
    "                line_buffer = \"\"\n",
    "            if ((len(line_buffer)>84 and token and token[0] == \" \") or len(line_buffer)>100):\n",
    "                line_buffer = \"\"\n",
    "                yield \"\\n\"\n",
    "                token = \"  \" + token.lstrip()\n",
    "        yield buffer if return_buffer else token\n",
    "\n",
    "    ##最后将聊天内容保存到对话内存缓冲区中\n",
    "    save_memory_and_get_output({'input':  message, 'output': buffer}, convstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://0.0.0.0:5000\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:5000/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'test', 'history': '', 'context': '[Quote from Document] {\\'Published\\': \\'2024-07-05\\', \\'Title\\': \\'SQLaser: Detecting DBMS Logic Bugs with Clause-Guided Fuzzing\\', \\'Authors\\': \\'Jin Wei, Ping Chen, Kangjie Lu, Jun Dai, Xiaoyan Sun\\', \\'Summary\\': \\'Database Management Systems (DBMSs) are vital components in modern\\\\ndata-driven systems. Their complexity often leads to logic bugs, which are\\\\nimplementation errors within the DBMSs that can lead to incorrect query\\\\nresults, data exposure, unauthorized access, etc., without necessarily causing\\\\nvisible system failures. Existing detection employs two strategies: rule-based\\\\nbug detection and coverage-guided fuzzing. In general, rule specification\\\\nitself is challenging; as a result, rule-based detection is limited to specific\\\\nand simple rules. Coverage-guided fuzzing blindly explores code paths or\\\\nblocks, many of which are unlikely to contain logic bugs; therefore, this\\\\nstrategy is cost-ineffective. In this paper, we design SQLaser, a\\\\nSQL-clause-guided fuzzer for detecting logic bugs in DBMSs. Through a\\\\ncomprehensive examination of most existing logic bugs across four distinct\\\\nDBMSs, excluding those causing system crashes, we have identified 35 logic bug\\\\npatterns. These patterns manifest as certain SQL clause combinations that\\\\ncommonly result in logic bugs, and behind these clause combinations are a\\\\nsequence of functions. We therefore model logic bug patterns as error-prone\\\\nfunction chains (ie, sequences of functions). We further develop a directed\\\\nfuzzer with a new path-to-path distance-calculation mechanism for effectively\\\\ntesting these chains and discovering additional logic bugs. This mechanism\\\\nenables SQLaser to swiftly navigate to target sites and uncover potential bugs\\\\nemerging from these paths. Our evaluation, conducted on SQLite, MySQL,\\\\nPostgreSQL, and TiDB, demonstrates that SQLaser significantly accelerates bug\\\\ndiscovery compared to other fuzzing approaches, reducing detection time by\\\\napproximately 60%.\\'}\\n[Quote from Document] Available Documents:\\n - Pathfinder: Exploring Path Diversity for Assessing Internet Censorship Inconsistency\\n - SQLaser: Detecting DBMS Logic Bugs with Clause-Guided Fuzzing\\n - Jailbreak Attacks and Defenses Against Large Language Models: A Survey\\n - HuntFUZZ: Enhancing Error Handling Testing through Clustering Based Fuzzing\\n - A Unified Learn-to-Distort-Data Framework for Privacy-Utility Trade-off in Trustworthy Federated Learning\\n - Temporal fingerprints: Identity matching across fully encrypted domain\\n - Waterfall: Framework for Robust and Scalable Text Watermarking\\n - GoSurf: Identifying Software Supply Chain Attack Vectors in Go\\n - Secure Rewind and Discard on ARM Morello\\n - Blockchain-based PKI within a Corporate Organization: Advantages and Challenges\\n - An AI Architecture with the Capability to Classify and Explain Hardware Trojans\\n - Differentially Private Inductive Miner\\n - Agnostic Private Density Estimation via Stable List Decoding\\n - On Evaluating The Performance of Watermarked Machine-Generated Texts Under Adversarial Attacks\\n - K-Nearest Neighbor Classification over Semantically Secure Encrypted Relational Data\\n - Late Breaking Results: Fortifying Neural Networks: Safeguarding Against Adversarial Attacks with Stochastic Computing\\n - KESIC: Kerberos Extensions for Smart, IoT and CPS Devices\\n - Differentially Private Convex Approximation of Two-Layer ReLU Networks\\n[Quote from Document] {\\'Published\\': \\'2024-07-05\\', \\'Title\\': \\'GoSurf: Identifying Software Supply Chain Attack Vectors in Go\\', \\'Authors\\': \\'Carmine Cesarano, Vivi Andersson, Roberto Natella, Martin Monperrus\\', \\'Summary\\': \\'In Go, the widespread adoption of open-source software has led to a\\\\nflourishing ecosystem of third-party dependencies, which are often integrated\\\\ninto critical systems. However, the reuse of dependencies introduces\\\\nsignificant supply chain security risks, as a single compromised package can\\\\nhave cascading impacts. Existing supply chain attack taxonomies overlook\\\\nlanguage-specific features that can be exploited by attackers to hide malicious\\\\ncode. In this paper, we propose a novel taxonomy of 12 distinct attack vectors\\\\ntailored for the Go language and its package lifecycle. Our taxonomy identifies\\\\npatterns in which language-specific Go features, intended for benign purposes,\\\\ncan be misused to propagate malicious code stealthily through supply chains.\\\\nAdditionally, we introduce GoSurf, a static analysis tool that analyzes the\\\\nattack surface of Go packages according to our proposed taxonomy. We evaluate\\\\nGoSurf on a corpus of widely used, real-world Go packages. Our work provides\\\\npreliminary insights for securing the open-source software supply chain within\\\\nthe Go ecosystem, allowing developers and security analysts to prioritize code\\\\naudit efforts and uncover hidden malicious behaviors.\\'}\\n[Quote from Document] {\\'Published\\': \\'2024-07-05\\', \\'Title\\': \\'HuntFUZZ: Enhancing Error Handling Testing through Clustering Based Fuzzing\\', \\'Authors\\': \\'Jin Wei, Ping Chen, Jun Dai, Xiaoyan Sun, Zhihao Zhang, Chang Xu, Yi Wanga\\', \\'Summary\\': \"Testing a program\\'s capability to effectively handling errors is a\\\\nsignificant challenge, given that program errors are relatively uncommon. To\\\\nsolve this, Software Fault Injection (SFI)-based fuzzing integrates SFI and\\\\ntraditional fuzzing, injecting and triggering errors for testing (error\\\\nhandling) code. However, we observe that current SFI-based fuzzing approaches\\\\nhave overlooked the correlation between paths housing error points. In fact,\\\\nthe execution paths of error points often share common paths. Nonetheless,\\\\nFuzzers usually generate test cases repeatedly to test error points on commonly\\\\ntraversed paths. This practice can compromise the efficiency of the fuzzer(s).\\\\nThus, this paper introduces HuntFUZZ, a novel SFI-based fuzzing framework that\\\\naddresses the issue of redundant testing of error points with correlated paths.\\\\nSpecifically, HuntFUZZ clusters these correlated error points and utilizes\\\\nconcolic execution to compute constraints only for common paths within each\\\\ncluster. By doing so, we provide the fuzzer with efficient test cases to\\\\nexplore related error points with minimal redundancy. We evaluate HuntFUZZ on a\\\\ndiverse set of 42 applications, and HuntFUZZ successfully reveals 162 known\\\\nbugs, with 62 of them being related to error handling. Additionally, due to its\\\\nefficient error point detection method, HuntFUZZ discovers 7 unique zero-day\\\\nbugs, which are all missed by existing fuzzers. Furthermore, we compare\\\\nHuntFUZZ with 4 existing fuzzing approaches, including AFL, AFL++, AFLGo, and\\\\nEH-FUZZ. Our evaluation confirms that HuntFUZZ can cover a broader range of\\\\nerror points, and it exhibits better performance in terms of bug finding speed.\"}\\n'}\n",
      "Keyboard interruption in main thread... closing server.\n",
      "Closing server running on port: 5000\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "chatbot = gr.Chatbot(value = [[None, initial_msg]])\n",
    "demo = gr.ChatInterface(chat_gen, chatbot=chatbot).queue()\n",
    "\n",
    "try:\n",
    "    demo.launch(debug=True, share=False, show_api=False, server_port=5000, server_name=\"0.0.0.0\")\n",
    "    demo.close()\n",
    "except Exception as e:\n",
    "    demo.close()\n",
    "    print(e)\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmdeploy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
